{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d21b494",
   "metadata": {},
   "source": [
    "- write python code to generate parquet file similar to the file in folder datarange1. keep same keys name, TO values are asset values add few asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c705a48",
   "metadata": {},
   "source": [
    "- You can generate a Parquet file similar to the one in daterange1 using pandas and pyarrow. Below is Python code that creates a DataFrame with the same keys, uses a few different asset values for TO, and writes it to a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839420e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample parquet file with 50 rows of data\n",
    "import pandas as pd\n",
    "\n",
    "# Example asset values for TO\n",
    "asset_values = [10540337, 10032096, 20012001, 30045009]\n",
    "\n",
    "# Generate sample data\n",
    "data = []\n",
    "for i in range(50):\n",
    "    asset = asset_values[i % len(asset_values)]\n",
    "    row = {\n",
    "        \"time\": 1743532199 + i * 86400,\n",
    "        \"TO\": asset,\n",
    "        \"sensor1_min\": round(40 + i * 0.5, 2),\n",
    "        \"sensor1_max\": round(60 + i * 0.7, 2),\n",
    "        \"sensor2_min\": round(10 + (i % 3) * 0.5, 2),\n",
    "        \"sensor2_max\": round(98 + (i % 2), 2),\n",
    "        \"sensor2_mean\": round(50 + i * 0.6, 2),\n",
    "        \"sensor3_min\": round(10 + (i % 5), 2),\n",
    "        \"sensor3_max\": round(99 - (i % 4), 2),\n",
    "        \"sensor3_mean\": round(52 + i * 0.4, 2)\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write to Parquet file\n",
    "df.to_parquet(\"../data/artifacts1/daterange1/training_data1_generated.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0c163",
   "metadata": {},
   "source": [
    "- thank you, this was good. now in datarange1, i have different parquet file. i want to read data for each and create a dataframe containing data from all files. add check in code if all file has same columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64b89a",
   "metadata": {},
   "source": [
    "- You can read all Parquet files in daterange1, check if all have the same columns, and concatenate them into a single DataFrame. Hereâ€™s Python code for your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44c3075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_data1_generated.parquet']\n",
      "(50, 10)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = \"../data/artifacts1/daterange1\"\n",
    "parquet_files = [f for f in os.listdir(folder) if f.endswith(\".parquet\")]\n",
    "print(parquet_files)\n",
    "\n",
    "dfs = []\n",
    "columns_set = set()\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(folder, file))\n",
    "    dfs.append(df)\n",
    "    columns_set.add(tuple(df.columns))\n",
    "\n",
    "if len(columns_set) > 1:\n",
    "    raise ValueError(\"Not all files have the same columns!\")\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d112744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>TO</th>\n",
       "      <th>sensor1_min</th>\n",
       "      <th>sensor1_max</th>\n",
       "      <th>sensor2_min</th>\n",
       "      <th>sensor2_max</th>\n",
       "      <th>sensor2_mean</th>\n",
       "      <th>sensor3_min</th>\n",
       "      <th>sensor3_max</th>\n",
       "      <th>sensor3_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743532199</td>\n",
       "      <td>10540337</td>\n",
       "      <td>40.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743618599</td>\n",
       "      <td>10032096</td>\n",
       "      <td>40.5</td>\n",
       "      <td>60.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>99.0</td>\n",
       "      <td>50.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>52.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1743704999</td>\n",
       "      <td>20012001</td>\n",
       "      <td>41.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>51.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1743791399</td>\n",
       "      <td>30045009</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>51.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>53.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1743877799</td>\n",
       "      <td>10540337</td>\n",
       "      <td>42.0</td>\n",
       "      <td>62.8</td>\n",
       "      <td>10.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>52.4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>53.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time        TO  sensor1_min  sensor1_max  sensor2_min  sensor2_max  \\\n",
       "0  1743532199  10540337         40.0         60.0         10.0         98.0   \n",
       "1  1743618599  10032096         40.5         60.7         10.5         99.0   \n",
       "2  1743704999  20012001         41.0         61.4         11.0         98.0   \n",
       "3  1743791399  30045009         41.5         62.1         10.0         99.0   \n",
       "4  1743877799  10540337         42.0         62.8         10.5         98.0   \n",
       "\n",
       "   sensor2_mean  sensor3_min  sensor3_max  sensor3_mean  \n",
       "0          50.0         10.0         99.0          52.0  \n",
       "1          50.6         11.0         98.0          52.4  \n",
       "2          51.2         12.0         97.0          52.8  \n",
       "3          51.8         13.0         96.0          53.2  \n",
       "4          52.4         14.0         99.0          53.6  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4d83cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/artifacts1/daterange1/training_data1_generated.parquet', '../data/artifacts1/daterange2/training_data1_generated.parquet']\n",
      "column : set()\n",
      "file : ../data/artifacts1/daterange1/training_data1_generated.parquet\n",
      "df columns : ('time', 'TO', 'sensor1_min', 'sensor1_max', 'sensor2_min', 'sensor2_max', 'sensor2_mean', 'sensor3_min', 'sensor3_max', 'sensor3_mean')\n",
      "column : set()\n",
      "column : {('time', 'TO', 'sensor1_min', 'sensor1_max', 'sensor2_min', 'sensor2_max', 'sensor2_mean', 'sensor3_min', 'sensor3_max', 'sensor3_mean')}\n",
      "columns set length:  1\n",
      "file : ../data/artifacts1/daterange2/training_data1_generated.parquet\n",
      "df columns : ('time', 'TO', 'sensor1_min', 'sensor1_max', 'sensor2_min', 'sensor2_max', 'sensor2_mean', 'sensor3_min', 'sensor3_max', 'sensor3_mean')\n",
      "column : {('time', 'TO', 'sensor1_min', 'sensor1_max', 'sensor2_min', 'sensor2_max', 'sensor2_mean', 'sensor3_min', 'sensor3_max', 'sensor3_mean')}\n",
      "column : {('time', 'TO', 'sensor1_min', 'sensor1_max', 'sensor2_min', 'sensor2_max', 'sensor2_mean', 'sensor3_min', 'sensor3_max', 'sensor3_mean')}\n",
      "columns set length:  1\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folders = [\"../data/artifacts1/daterange1\", \"../data/artifacts1/daterange2\"]\n",
    "parquet_files = []\n",
    "for folder in folders:\n",
    "    parquet_files += [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".parquet\")]\n",
    "print(parquet_files)\n",
    "\n",
    "dfs = []\n",
    "columns_set = set()\n",
    "print(\"column :\", columns_set)\n",
    "for file in parquet_files:\n",
    "    print(\"file :\", file)\n",
    "    df = pd.read_parquet(file)\n",
    "    dfs.append(df)\n",
    "    print(\"df columns :\", tuple(df.columns))\n",
    "    print(\"column :\", columns_set)\n",
    "    columns_set.add(tuple(df.columns))\n",
    "    print(\"column :\", columns_set)\n",
    "    print(\"columns set length: \",len(columns_set))\n",
    "\n",
    "if len(columns_set) > 1:\n",
    "    raise ValueError(\"Not all files have the same columns!\")\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b44fa078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/artifacts/daterange1/training_data1_generated.parquet',\n",
       " '../data/artifacts/daterange2/training_data1_generated.parquet']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be23f5",
   "metadata": {},
   "source": [
    "- thank you, this was really good, now i have parquet files in folder datarange1 and datarange2, modify above code so that it reads parquet file from multiple folders with previous task\n",
    "\n",
    "-  in folder artifacts, i have zip files, now modify code so that all parquet files in zip files are read and combined as previously\n",
    "\n",
    "- You can use Python's zipfile module to read Parquet files directly from zip archives without extracting them. Hereâ€™s how you can modify your code to read all Parquet files from zip files in the artifacts folder and combine them, checking for column consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7838ee3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip files:  ['reqid002.zip', 'reqid_001.zip']\n",
      "Processing reqid002.zip...\n",
      "../data/artifacts1/reqid002.zip\n",
      "  Found file in zip: reqid002/\n",
      "  Found file in zip: reqid002/1757527232_1757278782.parquet\n",
      "  Found file in zip: reqid002/1757527232_1757278781.parquet\n",
      "Processing reqid_001.zip...\n",
      "../data/artifacts1/reqid_001.zip\n",
      "  Found file in zip: reqid_001/\n",
      "  Found file in zip: reqid_001/1757565217_1757565282.parquet\n",
      "  Found file in zip: reqid_001/1757565217_1757565295.parquet\n",
      "  Found file in zip: reqid_001/1757527232_1757278522.parquet\n"
     ]
    }
   ],
   "source": [
    "# Now, let's read parquet files from zip archives\n",
    "#\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "artifacts_folder = \"../data/artifacts1\"\n",
    "dfs = []\n",
    "columns_set = set()\n",
    "\n",
    "# Find all zip files in the artifacts folder\n",
    "zip_files = [f for f in os.listdir(artifacts_folder) if f.endswith(\".zip\")]\n",
    "print(\"zip files: \", zip_files)\n",
    "\n",
    "for zip_filename in zip_files:\n",
    "    print(f\"Processing {zip_filename}...\")\n",
    "    zip_path = os.path.join(artifacts_folder, zip_filename)\n",
    "    print(zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        for file in z.namelist():\n",
    "            print(f\"  Found file in zip: {file}\")\n",
    "    #         if file.endswith(\".parquet\"):\n",
    "    #             print(f\"  Found parquet file: {file}\")\n",
    "    #             # with z.open(file) as parquet_file:\n",
    "    #             #     df = pd.read_parquet(BytesIO(parquet_file.read()))\n",
    "    #             #     dfs.append(df)\n",
    "    #             #     columns_set.add(tuple(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a843c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "### task: understand what is happening in the loop belowm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedab91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reqid002.zip...\n",
      "../data/artifacts1/reqid002.zip\n",
      "Processing reqid_001.zip...\n",
      "../data/artifacts1/reqid_001.zip\n",
      "(378, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "artifacts_folder = \"../data/artifacts1\"\n",
    "dfs = []\n",
    "columns_set = set()\n",
    "\n",
    "# Find all zip files in the artifacts folder\n",
    "zip_files = [f for f in os.listdir(artifacts_folder) if f.endswith(\".zip\")]\n",
    "\n",
    "for zip_filename in zip_files:\n",
    "    print(f\"Processing {zip_filename}...\")\n",
    "    zip_path = os.path.join(artifacts_folder, zip_filename)\n",
    "    print(zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        for file in z.namelist():\n",
    "            if file.endswith(\".parquet\"):\n",
    "                with z.open(file) as f:\n",
    "                    df = pd.read_parquet(BytesIO(f.read()))\n",
    "                    dfs.append(df)\n",
    "                    columns_set.add(tuple(df.columns))\n",
    "\n",
    "if len(columns_set) > 1:\n",
    "    raise ValueError(\"Not all files have the same columns!\")\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10f927c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/artifacts/reqid002.zip...\n",
      "../data/artifacts/reqid002.zip\n",
      "  Found parquet file: reqid002/1757527232_1757278782.parquet\n",
      "  Found parquet file: reqid002/1757527232_1757278781.parquet\n",
      "Processing ../data/artifacts/reqid_001.zip...\n",
      "../data/artifacts/reqid_001.zip\n",
      "  Found parquet file: reqid_001/1757565217_1757565282.parquet\n",
      "  Found parquet file: reqid_001/1757565217_1757565295.parquet\n",
      "  Found parquet file: reqid_001/1757527232_1757278522.parquet\n",
      "(378, 10)\n",
      "         time        TO  sensor1_min  sensor1_max  sensor2_min  sensor2_max  \\\n",
      "0  1743532199  10540337        65.83        65.27         10.0         98.0   \n",
      "1  1743618599  10540337        76.09        54.77         10.0         98.0   \n",
      "2  1743704999  10540337        67.66        66.12         10.0         99.0   \n",
      "3  1743791399  10540337        83.22        80.61         10.0         99.0   \n",
      "4  1743877799  10540337        78.29        66.14         10.0         99.0   \n",
      "\n",
      "   sensor2_mean  sensor3_min  sensor3_max  sensor3_mean  \n",
      "0         55.59         12.0         99.0         54.11  \n",
      "1         57.70         10.0         99.0         51.68  \n",
      "2         50.29         10.0         98.0         53.25  \n",
      "3         58.97         11.0         98.0         49.92  \n",
      "4         59.60         10.0         99.0         61.02  \n"
     ]
    }
   ],
   "source": [
    "# Now, let's read parquet files from zip archives using pathlib\n",
    "# \n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = Path(\"../data/artifacts\")\n",
    "dfs = []\n",
    "columns_set = set()\n",
    "\n",
    "# Find all zip files in the artifacts folder\n",
    "zip_files = list(folder_path.rglob(\"**/*.zip\"))\n",
    "for zip_path in zip_files:\n",
    "    print(f\"Processing {zip_path}...\")\n",
    "    # zip_path = os.path.join(artifacts_folder, zip_filename)\n",
    "    print(zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        for file in z.namelist():\n",
    "            if file.endswith(\".parquet\"):\n",
    "                print(f\"  Found parquet file: {file}\")\n",
    "                with z.open(file) as f:\n",
    "                    df = pd.read_parquet(BytesIO(f.read()))\n",
    "                    dfs.append(df)\n",
    "                    columns_set.add(tuple(df.columns))\n",
    "\n",
    "if len(columns_set) > 1:\n",
    "    raise ValueError(\"Not all files have the same columns!\")\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(combined_df.shape)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7843f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
