{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e616eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af5ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6541891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the parquet file from artifact3/reqid_001 folder\n",
    "df = pd.read_parquet('../data/artifacts3/reqid_001/1757527232_1757278522.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe99b868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>managedObjectId</th>\n",
       "      <th>measuringNodeId</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>indicator1_MIN</th>\n",
       "      <th>indicator1_MAX</th>\n",
       "      <th>indicator2_MIN</th>\n",
       "      <th>indicator2_MAX</th>\n",
       "      <th>indicator2_AVG</th>\n",
       "      <th>indicator3_MIN</th>\n",
       "      <th>indicator3_MAX</th>\n",
       "      <th>indicator3_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743532199</td>\n",
       "      <td>1743532199</td>\n",
       "      <td>65.83</td>\n",
       "      <td>65.27</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>55.59</td>\n",
       "      <td>12.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>54.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743618599</td>\n",
       "      <td>1743618599</td>\n",
       "      <td>76.09</td>\n",
       "      <td>54.77</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>57.70</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>51.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743704999</td>\n",
       "      <td>1743704999</td>\n",
       "      <td>67.66</td>\n",
       "      <td>66.12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>50.29</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>53.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743791399</td>\n",
       "      <td>1743791399</td>\n",
       "      <td>83.22</td>\n",
       "      <td>80.61</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>58.97</td>\n",
       "      <td>11.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743877799</td>\n",
       "      <td>1743877799</td>\n",
       "      <td>78.29</td>\n",
       "      <td>66.14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>59.60</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>61.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   managedObjectId  measuringNodeId       start         end  indicator1_MIN  \\\n",
       "0         10540337         10540337  1743532199  1743532199           65.83   \n",
       "1         10540337         10540337  1743618599  1743618599           76.09   \n",
       "2         10540337         10540337  1743704999  1743704999           67.66   \n",
       "3         10540337         10540337  1743791399  1743791399           83.22   \n",
       "4         10540337         10540337  1743877799  1743877799           78.29   \n",
       "\n",
       "   indicator1_MAX  indicator2_MIN  indicator2_MAX  indicator2_AVG  \\\n",
       "0           65.27            10.0            98.0           55.59   \n",
       "1           54.77            10.0            98.0           57.70   \n",
       "2           66.12            10.0            99.0           50.29   \n",
       "3           80.61            10.0            99.0           58.97   \n",
       "4           66.14            10.0            99.0           59.60   \n",
       "\n",
       "   indicator3_MIN  indicator3_MAX  indicator3_AVG  \n",
       "0            12.0            99.0           54.11  \n",
       "1            10.0            99.0           51.68  \n",
       "2            10.0            98.0           53.25  \n",
       "3            11.0            98.0           49.92  \n",
       "4            10.0            99.0           61.02  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd59b387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['managedObjectId', 'measuringNodeId', 'start', 'end', 'indicator1_MIN',\n",
       "       'indicator1_MAX', 'indicator2_MIN', 'indicator2_MAX', 'indicator2_AVG',\n",
       "       'indicator3_MIN', 'indicator3_MAX', 'indicator3_AVG'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4912df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the above parquet file for indicator1 and indicator2 into two different files\n",
    "df_indicator1 = df[df['indicator'] == 'indicator1']\n",
    "df_indicator2 = df[df['indicator'] == 'indicator2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### split parquet file based on schema split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1fa7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sensor1 columns\n",
    "sensor1_cols = [col for col in df.columns if col.startswith(\"indicator1_\")]\n",
    "sensor1_df = df[[\"managedObjectId\", \"measuringNodeId\", \"start\", \"end\"] + sensor1_cols]\n",
    "\n",
    "# Select remaining columns (excluding sensor1 columns)\n",
    "remaining_cols = [col for col in df.columns if col not in sensor1_cols]\n",
    "remaining_cols = [col for col in remaining_cols if col not in [\"managedObjectId\", \"measuringNodeId\", \"start\", \"end\"]]\n",
    "remaining_df = df[[\"managedObjectId\", \"measuringNodeId\", \"start\", \"end\"] + remaining_cols]\n",
    "\n",
    "# Write to separate Parquet files\n",
    "sensor1_df.to_parquet(\"../data/artifacts3/reqid_001/1757527256_1757278582_sensor1.parquet\", index=False)\n",
    "remaining_df.to_parquet(\"../data/artifacts3/reqid_001/1757527256_1757278582_remaining.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d9d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a25c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part files in ../data/artifacts3/reqid_001: ['1757527256_1757278582_part_002.parquet', '1757527256_1757278582_part_001.parquet']\n",
      "Full files in ../data/artifacts3/reqid_001: ['1757565217_1757565282.parquet', '1757565217_1757565295.parquet', '1757527232_1757278522.parquet']\n",
      "f:  1757527256_1757278582_part_002.parquet\n",
      "inside for loop, folder:  ../data/artifacts3/reqid_001\n",
      "f:  1757527256_1757278582_part_001.parquet\n",
      "inside for loop, folder:  ../data/artifacts3/reqid_001\n",
      "Part files in ../data/artifacts3/reqid_002: []\n",
      "Full files in ../data/artifacts3/reqid_002: ['1757565217_1757565295.parquet']\n",
      "Part files in ../data/artifacts3/reqid_003: []\n",
      "Full files in ../data/artifacts3/reqid_003: ['1757565217_1757565282.parquet', '1757527232_1757278522.parquet']\n",
      "(574, 12)\n",
      "   managedObjectId  measuringNodeId       start         end  indicator1_MIN  \\\n",
      "0         10540337         10540337  1743532199  1743532199           65.83   \n",
      "1         10540337         10540337  1743618599  1743618599           76.09   \n",
      "2         10540337         10540337  1743704999  1743704999           67.66   \n",
      "3         10540337         10540337  1743791399  1743791399           83.22   \n",
      "4         10540337         10540337  1743877799  1743877799           78.29   \n",
      "\n",
      "   indicator1_MAX  indicator2_MIN  indicator2_MAX  indicator2_AVG  \\\n",
      "0           65.27            10.0            98.0           55.59   \n",
      "1           54.77            10.0            98.0           57.70   \n",
      "2           66.12            10.0            99.0           50.29   \n",
      "3           80.61            10.0            99.0           58.97   \n",
      "4           66.14            10.0            99.0           59.60   \n",
      "\n",
      "   indicator3_MIN  indicator3_MAX  indicator3_AVG  \n",
      "0            12.0            99.0           54.11  \n",
      "1            10.0            99.0           51.68  \n",
      "2            10.0            98.0           53.25  \n",
      "3            11.0            98.0           49.92  \n",
      "4            10.0            99.0           61.02  \n"
     ]
    }
   ],
   "source": [
    "# Now, let's read and merge parquet files from multiple folders and \n",
    "# maintain the order of operations for schema consistency\n",
    "# \n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "folders = [\"../data/artifacts3/reqid_001\", \"../data/artifacts3/reqid_002\",\n",
    "            \"../data/artifacts3/reqid_003\"]\n",
    "dfs = []\n",
    "\n",
    "for folder in folders:\n",
    "    parquet_files = [f for f in os.listdir(folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "    # Separate part files and full files\n",
    "    part_files = [f for f in parquet_files if \"_part\" in f]\n",
    "    print(f\"Part files in {folder}:\", part_files)\n",
    "    full_files = [f for f in parquet_files if \"_part\" not in f]\n",
    "    print(f\"Full files in {folder}:\", full_files)\n",
    "\n",
    "    # Handle full files\n",
    "    for f in full_files:\n",
    "        df = pd.read_parquet(os.path.join(folder, f))\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Handle part files: group by prefix before '_part'\n",
    "    part_groups = defaultdict(list)\n",
    "    for f in part_files:\n",
    "        print(\"f: \", f)\n",
    "        print(\"inside for loop, folder: \", folder)\n",
    "        prefix = f.split(\"_part\")[0]\n",
    "        part_groups[prefix].append(f)\n",
    "\n",
    "    for prefix, files in part_groups.items():\n",
    "        # Sort to ensure consistent order\n",
    "        files = sorted(files)\n",
    "        part_dfs = [pd.read_parquet(os.path.join(folder, f)) for f in files]\n",
    "        # Merge column-wise (axis=1)\n",
    "        merged_df = pd.concat(part_dfs, axis=1)\n",
    "        # Remove duplicate columns if any (e.g., 'time', 'TO')\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "        dfs.append(merged_df)\n",
    "\n",
    "# Combine all data vertically (axis=0)\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(final_df.shape)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1db5264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>managedObjectId</th>\n",
       "      <th>measuringNodeId</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>indicator1_MIN</th>\n",
       "      <th>indicator1_MAX</th>\n",
       "      <th>indicator2_MIN</th>\n",
       "      <th>indicator2_MAX</th>\n",
       "      <th>indicator2_AVG</th>\n",
       "      <th>indicator3_MIN</th>\n",
       "      <th>indicator3_MAX</th>\n",
       "      <th>indicator3_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743532199</td>\n",
       "      <td>1743532199</td>\n",
       "      <td>65.83</td>\n",
       "      <td>65.27</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>55.59</td>\n",
       "      <td>12.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>54.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743618599</td>\n",
       "      <td>1743618599</td>\n",
       "      <td>76.09</td>\n",
       "      <td>54.77</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>57.70</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>51.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743704999</td>\n",
       "      <td>1743704999</td>\n",
       "      <td>67.66</td>\n",
       "      <td>66.12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>50.29</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>53.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743791399</td>\n",
       "      <td>1743791399</td>\n",
       "      <td>83.22</td>\n",
       "      <td>80.61</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>58.97</td>\n",
       "      <td>11.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10540337</td>\n",
       "      <td>10540337</td>\n",
       "      <td>1743877799</td>\n",
       "      <td>1743877799</td>\n",
       "      <td>78.29</td>\n",
       "      <td>66.14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>59.60</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>61.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   managedObjectId  measuringNodeId       start         end  indicator1_MIN  \\\n",
       "0         10540337         10540337  1743532199  1743532199           65.83   \n",
       "1         10540337         10540337  1743618599  1743618599           76.09   \n",
       "2         10540337         10540337  1743704999  1743704999           67.66   \n",
       "3         10540337         10540337  1743791399  1743791399           83.22   \n",
       "4         10540337         10540337  1743877799  1743877799           78.29   \n",
       "\n",
       "   indicator1_MAX  indicator2_MIN  indicator2_MAX  indicator2_AVG  \\\n",
       "0           65.27            10.0            98.0           55.59   \n",
       "1           54.77            10.0            98.0           57.70   \n",
       "2           66.12            10.0            99.0           50.29   \n",
       "3           80.61            10.0            99.0           58.97   \n",
       "4           66.14            10.0            99.0           59.60   \n",
       "\n",
       "   indicator3_MIN  indicator3_MAX  indicator3_AVG  \n",
       "0            12.0            99.0           54.11  \n",
       "1            10.0            99.0           51.68  \n",
       "2            10.0            98.0           53.25  \n",
       "3            11.0            98.0           49.92  \n",
       "4            10.0            99.0           61.02  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c83a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9607fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip files found: ['reqid_002.zip', 'reqid_003.zip', 'reqid_001.zip']\n",
      "Processing ../data/artifacts3/reqid_002.zip...\n",
      "Files in zip: ['reqid_002/', 'reqid_002/1757565217_1757565295.parquet']\n",
      "Processing ../data/artifacts3/reqid_003.zip...\n",
      "Files in zip: ['reqid_003/', 'reqid_003/1757565217_1757565282.parquet', 'reqid_003/1757527232_1757278522.parquet']\n",
      "Processing ../data/artifacts3/reqid_001.zip...\n",
      "Files in zip: ['reqid_001/', 'reqid_001/1757565217_1757565282.parquet', 'reqid_001/1757527256_1757278582_part_002.parquet', 'reqid_001/1757527256_1757278582_part_001.parquet', 'reqid_001/1757565217_1757565295.parquet', 'reqid_001/1757527232_1757278522.parquet']\n",
      "Processing part group 1757527256_1757278582 with files: ['reqid_001/1757527256_1757278582_part_001.parquet', 'reqid_001/1757527256_1757278582_part_002.parquet']\n",
      "(574, 12)\n",
      "   managedObjectId  measuringNodeId       start         end  indicator1_MIN  \\\n",
      "0         10540337         10540337  1743532199  1743532199           65.83   \n",
      "1         10540337         10540337  1743618599  1743618599           76.09   \n",
      "2         10540337         10540337  1743704999  1743704999           67.66   \n",
      "3         10540337         10540337  1743791399  1743791399           83.22   \n",
      "4         10540337         10540337  1743877799  1743877799           78.29   \n",
      "\n",
      "   indicator1_MAX  indicator2_MIN  indicator2_MAX  indicator2_AVG  \\\n",
      "0           65.27            10.0            98.0           55.59   \n",
      "1           54.77            10.0            98.0           57.70   \n",
      "2           66.12            10.0            99.0           50.29   \n",
      "3           80.61            10.0            99.0           58.97   \n",
      "4           66.14            10.0            99.0           59.60   \n",
      "\n",
      "   indicator3_MIN  indicator3_MAX  indicator3_AVG  \n",
      "0            12.0            99.0           54.11  \n",
      "1            10.0            99.0           51.68  \n",
      "2            10.0            98.0           53.25  \n",
      "3            11.0            98.0           49.92  \n",
      "4            10.0            99.0           61.02  \n"
     ]
    }
   ],
   "source": [
    "# Now, let's read and merge parquet files from multiple zip files in a folder and \n",
    "# maintain the order of operations for schema consistency\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from collections import defaultdict\n",
    "\n",
    "artifacts_folder = \"../data/artifacts3\"\n",
    "dfs = []\n",
    "\n",
    "# Find all zip files in the artifacts folder\n",
    "zip_files = [f for f in os.listdir(artifacts_folder) if f.endswith(\".zip\")]\n",
    "print(\"Zip files found:\", zip_files)\n",
    "\n",
    "for zip_filename in zip_files:\n",
    "    zip_path = os.path.join(artifacts_folder, zip_filename)\n",
    "    print(f\"Processing {zip_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        print(\"Files in zip:\", z.namelist())\n",
    "        # Group part files by prefix, collect full files\n",
    "        part_groups = defaultdict(list)\n",
    "        full_files = []\n",
    "        for file in z.namelist():\n",
    "            if file.endswith(\".parquet\"):\n",
    "                if \"_part\" in os.path.basename(file):\n",
    "                    prefix = os.path.basename(file).split(\"_part\")[0]\n",
    "                    part_groups[prefix].append(file)\n",
    "                else:\n",
    "                    full_files.append(file)\n",
    "        # Handle full files\n",
    "        for file in full_files:\n",
    "            with z.open(file) as f:\n",
    "                df = pd.read_parquet(BytesIO(f.read()))\n",
    "                dfs.append(df)\n",
    "        # Handle part files\n",
    "        for prefix, files in part_groups.items():\n",
    "            files = sorted(files)\n",
    "            print(f\"Processing part group {prefix} with files: {files}\")\n",
    "            part_dfs = []\n",
    "            for file in files:\n",
    "                with z.open(file) as f:\n",
    "                    part_dfs.append(pd.read_parquet(BytesIO(f.read())))\n",
    "            merged_df = pd.concat(part_dfs, axis=1)\n",
    "            merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "            dfs.append(merged_df)\n",
    "\n",
    "# Combine all data vertically (axis=0)\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(final_df.shape)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### what will be impact of zip file order not being maintained while merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba436d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is code for merging parquet file from different zip files. inside zip file some of the parquet files are based on schema split. this case is also handeled. Now i want to translate this code to production standard code with small methods, definition. break the code in such a way so that i can take this to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49ba39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "def find_zip_files(folder: str) -> List[str]:\n",
    "    \"\"\"Return list of zip files in the given folder.\"\"\"\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".zip\")]\n",
    "\n",
    "def group_parquet_files(zip_file: zipfile.ZipFile) -> (List[str], dict):\n",
    "    \"\"\"Group parquet files in zip into full files and part groups.\"\"\"\n",
    "    part_groups = defaultdict(list)\n",
    "    full_files = []\n",
    "    for file in zip_file.namelist():\n",
    "        if file.endswith(\".parquet\"):\n",
    "            if \"_part\" in os.path.basename(file):\n",
    "                prefix = os.path.basename(file).split(\"_part\")[0]\n",
    "                part_groups[prefix].append(file)\n",
    "            else:\n",
    "                full_files.append(file)\n",
    "    return full_files, part_groups\n",
    "\n",
    "def read_parquet_from_zip(zip_file: zipfile.ZipFile, file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read a parquet file from a zip archive.\"\"\"\n",
    "    with zip_file.open(file) as f:\n",
    "        return pd.read_parquet(BytesIO(f.read()))\n",
    "\n",
    "def merge_part_files(zip_file: zipfile.ZipFile, files: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Merge part parquet files column-wise, removing duplicate columns.\"\"\"\n",
    "    part_dfs = [read_parquet_from_zip(zip_file, file) for file in sorted(files)]\n",
    "    merged_df = pd.concat(part_dfs, axis=1)\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "    return merged_df\n",
    "\n",
    "def load_all_parquet_from_zips(artifacts_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and merge all parquet files from all zip files in a folder.\"\"\"\n",
    "    dfs = []\n",
    "    zip_files = find_zip_files(artifacts_folder)\n",
    "    print(\"Zip files found:\", zip_files)\n",
    "    for zip_path in zip_files:\n",
    "        print(f\"Processing {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            print(\"Files in zip:\", z.namelist())\n",
    "            full_files, part_groups = group_parquet_files(z)\n",
    "            # Handle full files\n",
    "            for file in full_files:\n",
    "                dfs.append(read_parquet_from_zip(z, file))\n",
    "            # Handle part files\n",
    "            for prefix, files in part_groups.items():\n",
    "                print(f\"Processing part group {prefix} with files: {files}\")\n",
    "                dfs.append(merge_part_files(z, files))\n",
    "    if not dfs:\n",
    "        raise ValueError(\"No parquet data found in any zip files.\")\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf14d420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip files found: ['../data/artifacts3/reqid_002.zip', '../data/artifacts3/reqid_003.zip', '../data/artifacts3/reqid_001.zip']\n",
      "Processing ../data/artifacts3/reqid_002.zip...\n",
      "Files in zip: ['reqid_002/', 'reqid_002/1757565217_1757565295.parquet']\n",
      "Processing ../data/artifacts3/reqid_003.zip...\n",
      "Files in zip: ['reqid_003/', 'reqid_003/1757565217_1757565282.parquet', 'reqid_003/1757527232_1757278522.parquet']\n",
      "Processing ../data/artifacts3/reqid_001.zip...\n",
      "Files in zip: ['reqid_001/', 'reqid_001/1757565217_1757565282.parquet', 'reqid_001/1757527256_1757278582_part_002.parquet', 'reqid_001/1757527256_1757278582_part_001.parquet', 'reqid_001/1757565217_1757565295.parquet', 'reqid_001/1757527232_1757278522.parquet']\n",
      "Processing part group 1757527256_1757278582 with files: ['reqid_001/1757527256_1757278582_part_002.parquet', 'reqid_001/1757527256_1757278582_part_001.parquet']\n",
      "(574, 12)\n",
      "   managedObjectId  measuringNodeId       start         end  indicator1_MIN  \\\n",
      "0         10540337         10540337  1743532199  1743532199           65.83   \n",
      "1         10540337         10540337  1743618599  1743618599           76.09   \n",
      "2         10540337         10540337  1743704999  1743704999           67.66   \n",
      "3         10540337         10540337  1743791399  1743791399           83.22   \n",
      "4         10540337         10540337  1743877799  1743877799           78.29   \n",
      "\n",
      "   indicator1_MAX  indicator2_MIN  indicator2_MAX  indicator2_AVG  \\\n",
      "0           65.27            10.0            98.0           55.59   \n",
      "1           54.77            10.0            98.0           57.70   \n",
      "2           66.12            10.0            99.0           50.29   \n",
      "3           80.61            10.0            99.0           58.97   \n",
      "4           66.14            10.0            99.0           59.60   \n",
      "\n",
      "   indicator3_MIN  indicator3_MAX  indicator3_AVG  \n",
      "0            12.0            99.0           54.11  \n",
      "1            10.0            99.0           51.68  \n",
      "2            10.0            98.0           53.25  \n",
      "3            11.0            98.0           49.92  \n",
      "4            10.0            99.0           61.02  \n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "artifacts_folder = \"../data/artifacts3\"\n",
    "final_df = load_all_parquet_from_zips(artifacts_folder)\n",
    "print(final_df.shape)\n",
    "print(final_df.head())\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e1ed7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
