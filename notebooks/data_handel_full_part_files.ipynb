{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c1b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a parquet file into two based on column names\n",
    "import pandas as pd\n",
    "\n",
    "# Read the original Parquet file\n",
    "df = pd.read_parquet(\"../data/artifacts2/reqid_001/1757527232_1757278522.parquet\")\n",
    "\n",
    "# Select sensor1 columns\n",
    "sensor1_cols = [col for col in df.columns if col.startswith(\"sensor1_\")]\n",
    "sensor1_df = df[[\"time\", \"TO\"] + sensor1_cols]\n",
    "\n",
    "# Select remaining columns (excluding sensor1 columns)\n",
    "remaining_cols = [col for col in df.columns if col not in sensor1_cols]\n",
    "remaining_cols = [col for col in remaining_cols if col not in [\"time\", \"TO\"]]\n",
    "remaining_df = df[[\"time\", \"TO\"] + remaining_cols]\n",
    "\n",
    "# Write to separate Parquet files\n",
    "sensor1_df.to_parquet(\"../data/artifacts2/reqid_001/1757527232_1757278522_sensor1.parquet\", index=False)\n",
    "remaining_df.to_parquet(\"../data/artifacts2/reqid_001/1757527232_1757278522_remaining.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563d37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original Parquet file\n",
    "df = pd.read_parquet(\"../data/artifacts2/reqid_001/1757527232_1757278522.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d312a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>TO</th>\n",
       "      <th>sensor1_min</th>\n",
       "      <th>sensor1_max</th>\n",
       "      <th>sensor2_min</th>\n",
       "      <th>sensor2_max</th>\n",
       "      <th>sensor2_mean</th>\n",
       "      <th>sensor3_min</th>\n",
       "      <th>sensor3_max</th>\n",
       "      <th>sensor3_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743532199</td>\n",
       "      <td>10540337</td>\n",
       "      <td>65.83</td>\n",
       "      <td>65.27</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>55.59</td>\n",
       "      <td>12.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>54.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743618599</td>\n",
       "      <td>10540337</td>\n",
       "      <td>76.09</td>\n",
       "      <td>54.77</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>57.70</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>51.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1743704999</td>\n",
       "      <td>10540337</td>\n",
       "      <td>67.66</td>\n",
       "      <td>66.12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>50.29</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>53.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1743791399</td>\n",
       "      <td>10540337</td>\n",
       "      <td>83.22</td>\n",
       "      <td>80.61</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>58.97</td>\n",
       "      <td>11.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1743877799</td>\n",
       "      <td>10540337</td>\n",
       "      <td>78.29</td>\n",
       "      <td>66.14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>59.60</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>61.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time        TO  sensor1_min  sensor1_max  sensor2_min  sensor2_max  \\\n",
       "0  1743532199  10540337        65.83        65.27         10.0         98.0   \n",
       "1  1743618599  10540337        76.09        54.77         10.0         98.0   \n",
       "2  1743704999  10540337        67.66        66.12         10.0         99.0   \n",
       "3  1743791399  10540337        83.22        80.61         10.0         99.0   \n",
       "4  1743877799  10540337        78.29        66.14         10.0         99.0   \n",
       "\n",
       "   sensor2_mean  sensor3_min  sensor3_max  sensor3_mean  \n",
       "0         55.59         12.0         99.0         54.11  \n",
       "1         57.70         10.0         99.0         51.68  \n",
       "2         50.29         10.0         98.0         53.25  \n",
       "3         58.97         11.0         98.0         49.92  \n",
       "4         59.60         10.0         99.0         61.02  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09e55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor1_cols = [col for col in df.columns if col.startswith(\"sensor1_\")]\n",
    "sensor1_df = df[[\"time\", \"TO\"] + sensor1_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6250c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>TO</th>\n",
       "      <th>sensor1_min</th>\n",
       "      <th>sensor1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743532199</td>\n",
       "      <td>10540337</td>\n",
       "      <td>65.83</td>\n",
       "      <td>65.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743618599</td>\n",
       "      <td>10540337</td>\n",
       "      <td>76.09</td>\n",
       "      <td>54.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1743704999</td>\n",
       "      <td>10540337</td>\n",
       "      <td>67.66</td>\n",
       "      <td>66.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1743791399</td>\n",
       "      <td>10540337</td>\n",
       "      <td>83.22</td>\n",
       "      <td>80.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1743877799</td>\n",
       "      <td>10540337</td>\n",
       "      <td>78.29</td>\n",
       "      <td>66.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time        TO  sensor1_min  sensor1_max\n",
       "0  1743532199  10540337        65.83        65.27\n",
       "1  1743618599  10540337        76.09        54.77\n",
       "2  1743704999  10540337        67.66        66.12\n",
       "3  1743791399  10540337        83.22        80.61\n",
       "4  1743877799  10540337        78.29        66.14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e196f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select remaining columns (excluding sensor1 columns)\n",
    "remaining_cols = [col for col in df.columns if col not in sensor1_cols]\n",
    "remaining_cols = [col for col in remaining_cols if col not in [\"time\", \"TO\"]]\n",
    "remaining_df = df[[\"time\", \"TO\"] + remaining_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f9f242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>TO</th>\n",
       "      <th>sensor2_min</th>\n",
       "      <th>sensor2_max</th>\n",
       "      <th>sensor2_mean</th>\n",
       "      <th>sensor3_min</th>\n",
       "      <th>sensor3_max</th>\n",
       "      <th>sensor3_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743532199</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>55.59</td>\n",
       "      <td>12.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>54.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743618599</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>57.70</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>51.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1743704999</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>50.29</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>53.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1743791399</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>58.97</td>\n",
       "      <td>11.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1743877799</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>59.60</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>61.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time        TO  sensor2_min  sensor2_max  sensor2_mean  sensor3_min  \\\n",
       "0  1743532199  10540337         10.0         98.0         55.59         12.0   \n",
       "1  1743618599  10540337         10.0         98.0         57.70         10.0   \n",
       "2  1743704999  10540337         10.0         99.0         50.29         10.0   \n",
       "3  1743791399  10540337         10.0         99.0         58.97         11.0   \n",
       "4  1743877799  10540337         10.0         99.0         59.60         10.0   \n",
       "\n",
       "   sensor3_max  sensor3_mean  \n",
       "0         99.0         54.11  \n",
       "1         99.0         51.68  \n",
       "2         98.0         53.25  \n",
       "3         98.0         49.92  \n",
       "4         99.0         61.02  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c12d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1757527232_1757278522_part2.parquet', '1757527232_1757278522_part1.parquet']\n",
      "['1757565217_1757565282.parquet', '1757565217_1757565295.parquet', '1757527232_1757278522.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Now, let's read and merge parquet files from a folder\n",
    "# \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = \"../data/artifacts2/reqid_001\"\n",
    "parquet_files = [f for f in os.listdir(folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "# Separate part files and full files\n",
    "part_files = [f for f in parquet_files if \"_part\" in f]\n",
    "print(part_files)\n",
    "full_files = [f for f in parquet_files if \"_part\" not in f]\n",
    "print(full_files)\n",
    "\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "976d5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part groups:  defaultdict(<class 'list'>, {})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "part_groups = defaultdict(list)\n",
    "\n",
    "print(\"part groups: \", part_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488601ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:  1757527232_1757278522_part2.parquet\n",
      "part groups:  defaultdict(<class 'list'>, {'1757527232_1757278522': ['1757527232_1757278522_part2.parquet']})\n",
      "f:  1757527232_1757278522_part1.parquet\n",
      "part groups:  defaultdict(<class 'list'>, {'1757527232_1757278522': ['1757527232_1757278522_part2.parquet', '1757527232_1757278522_part1.parquet']})\n"
     ]
    }
   ],
   "source": [
    "for f in part_files:\n",
    "    print(\"f: \", f)\n",
    "    prefix = f.split(\"_part\")[0]\n",
    "    part_groups[prefix].append(f)\n",
    "    print(\"part groups: \", part_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72e3ddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files ['1757527232_1757278522_part1.parquet', '1757527232_1757278522_part2.parquet']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for prefix, files in part_groups.items():\n",
    "    # Sort to ensure consistent order\n",
    "    files = sorted(files)\n",
    "\n",
    "print(\"files\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b659137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part_dfs:  [          time        TO  sensor2_min  sensor2_max  sensor2_mean  sensor3_min  \\\n",
      "0   1743532199  10540337        10.00        98.00         55.59        12.00   \n",
      "1   1743618599  10540337        10.00        98.00         57.70        10.00   \n",
      "2   1743704999  10540337        10.00        99.00         50.29        10.00   \n",
      "3   1743791399  10540337        10.00        99.00         58.97        11.00   \n",
      "4   1743877799  10540337        10.00        99.00         59.60        10.00   \n",
      "..         ...       ...          ...          ...           ...          ...   \n",
      "77  1746728999  10540337         9.90        98.01         55.41        12.23   \n",
      "78  1746728999  10540337        10.59        97.39         56.05        11.32   \n",
      "79  1746815399  10540337        10.53        98.79         55.84        11.95   \n",
      "80  1746815399  10540337        10.66        99.02         54.12        12.53   \n",
      "81  1746815399  10540337        11.73        98.66         52.69        10.92   \n",
      "\n",
      "    sensor3_max  sensor3_mean  \n",
      "0         99.00         54.11  \n",
      "1         99.00         51.68  \n",
      "2         98.00         53.25  \n",
      "3         98.00         49.92  \n",
      "4         99.00         61.02  \n",
      "..          ...           ...  \n",
      "77        99.00         49.45  \n",
      "78        98.55         54.28  \n",
      "79        98.67         57.80  \n",
      "80        98.22         54.21  \n",
      "81        98.62         51.60  \n",
      "\n",
      "[82 rows x 8 columns],           time        TO  sensor1_min  sensor1_max\n",
      "0   1743532199  10540337        65.83        65.27\n",
      "1   1743618599  10540337        76.09        54.77\n",
      "2   1743704999  10540337        67.66        66.12\n",
      "3   1743791399  10540337        83.22        80.61\n",
      "4   1743877799  10540337        78.29        66.14\n",
      "..         ...       ...          ...          ...\n",
      "77  1746728999  10540337        81.77        70.40\n",
      "78  1746728999  10540337        96.03        29.12\n",
      "79  1746815399  10540337        30.57        38.09\n",
      "80  1746815399  10540337        47.62        54.37\n",
      "81  1746815399  10540337        26.44        78.26\n",
      "\n",
      "[82 rows x 4 columns]]\n"
     ]
    }
   ],
   "source": [
    "for prefix, files in part_groups.items():\n",
    "    # Sort to ensure consistent order\n",
    "    files = sorted(files)\n",
    "    part_dfs = [pd.read_parquet(os.path.join(folder, f)) for f in files]\n",
    "    print(\"part_dfs: \", part_dfs)\n",
    "    # # Merge column-wise (axis=1)\n",
    "    # merged_df = pd.concat(part_dfs, axis=1)\n",
    "    # # Remove duplicate columns if any (e.g., 'time', 'TO')\n",
    "    # merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "    # dfs.append(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa339884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_df:           time        TO  sensor2_min  sensor2_max  sensor2_mean  sensor3_min  \\\n",
      "0  1743532199  10540337         10.0         98.0         55.59         12.0   \n",
      "1  1743618599  10540337         10.0         98.0         57.70         10.0   \n",
      "2  1743704999  10540337         10.0         99.0         50.29         10.0   \n",
      "3  1743791399  10540337         10.0         99.0         58.97         11.0   \n",
      "4  1743877799  10540337         10.0         99.0         59.60         10.0   \n",
      "\n",
      "   sensor3_max  sensor3_mean        time        TO  sensor1_min  sensor1_max  \n",
      "0         99.0         54.11  1743532199  10540337        65.83        65.27  \n",
      "1         99.0         51.68  1743618599  10540337        76.09        54.77  \n",
      "2         98.0         53.25  1743704999  10540337        67.66        66.12  \n",
      "3         98.0         49.92  1743791399  10540337        83.22        80.61  \n",
      "4         99.0         61.02  1743877799  10540337        78.29        66.14  \n",
      "merged_df:           time        TO  sensor2_min  sensor2_max  sensor2_mean  sensor3_min  \\\n",
      "0  1743532199  10540337         10.0         98.0         55.59         12.0   \n",
      "1  1743618599  10540337         10.0         98.0         57.70         10.0   \n",
      "2  1743704999  10540337         10.0         99.0         50.29         10.0   \n",
      "3  1743791399  10540337         10.0         99.0         58.97         11.0   \n",
      "4  1743877799  10540337         10.0         99.0         59.60         10.0   \n",
      "\n",
      "   sensor3_max  sensor3_mean  sensor1_min  sensor1_max  \n",
      "0         99.0         54.11        65.83        65.27  \n",
      "1         99.0         51.68        76.09        54.77  \n",
      "2         98.0         53.25        67.66        66.12  \n",
      "3         98.0         49.92        83.22        80.61  \n",
      "4         99.0         61.02        78.29        66.14  \n"
     ]
    }
   ],
   "source": [
    "for prefix, files in part_groups.items():\n",
    "    # Sort to ensure consistent order\n",
    "    files = sorted(files)\n",
    "    part_dfs = [pd.read_parquet(os.path.join(folder, f)) for f in files]\n",
    "    # print(\"part_dfs: \", part_dfs)\n",
    "    # Merge column-wise (axis=1)\n",
    "    merged_df = pd.concat(part_dfs, axis=1)\n",
    "    print(\"merged_df: \", merged_df.head())\n",
    "    # Remove duplicate columns if any (e.g., 'time', 'TO')\n",
    "    merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "    # dfs.append(merged_df)\n",
    "    print(\"merged_df: \", merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cce9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445a9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d268ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1757527232_1757278522_part2.parquet', '1757527232_1757278522_part1.parquet']\n",
      "['1757565217_1757565282.parquet', '1757565217_1757565295.parquet', '1757527232_1757278522.parquet']\n",
      "f:  1757527232_1757278522_part2.parquet\n",
      "f:  1757527232_1757278522_part1.parquet\n",
      "(296, 10)\n"
     ]
    }
   ],
   "source": [
    "# Now, let's read and merge parquet files from a folder\n",
    "# \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = \"../data/artifacts2/reqid_001\"\n",
    "parquet_files = [f for f in os.listdir(folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "# Separate part files and full files\n",
    "part_files = [f for f in parquet_files if \"_part\" in f]\n",
    "print(part_files)\n",
    "full_files = [f for f in parquet_files if \"_part\" not in f]\n",
    "print(full_files)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Handle part files: group by prefix before '_part'\n",
    "from collections import defaultdict\n",
    "part_groups = defaultdict(list)\n",
    "for f in part_files:\n",
    "    print(\"f: \", f)\n",
    "    prefix = f.split(\"_part\")[0]\n",
    "    part_groups[prefix].append(f)\n",
    "\n",
    "for prefix, files in part_groups.items():\n",
    "    # Sort to ensure consistent order\n",
    "    files = sorted(files)\n",
    "    part_dfs = [pd.read_parquet(os.path.join(folder, f)) for f in files]\n",
    "    # Merge column-wise (axis=1)\n",
    "    merged_df = pd.concat(part_dfs, axis=1)\n",
    "    # Remove duplicate columns if any (e.g., 'time', 'TO')\n",
    "    merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "    dfs.append(merged_df)\n",
    "\n",
    "# Handle full files\n",
    "for f in full_files:\n",
    "    df = pd.read_parquet(os.path.join(folder, f))\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all data vertically (axis=0)\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b76b6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>TO</th>\n",
       "      <th>sensor2_min</th>\n",
       "      <th>sensor2_max</th>\n",
       "      <th>sensor2_mean</th>\n",
       "      <th>sensor3_min</th>\n",
       "      <th>sensor3_max</th>\n",
       "      <th>sensor3_mean</th>\n",
       "      <th>sensor1_min</th>\n",
       "      <th>sensor1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743532199</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>55.59</td>\n",
       "      <td>12.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>54.11</td>\n",
       "      <td>65.83</td>\n",
       "      <td>65.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743618599</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>57.70</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>51.68</td>\n",
       "      <td>76.09</td>\n",
       "      <td>54.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1743704999</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>50.29</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>53.25</td>\n",
       "      <td>67.66</td>\n",
       "      <td>66.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1743791399</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>58.97</td>\n",
       "      <td>11.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.92</td>\n",
       "      <td>83.22</td>\n",
       "      <td>80.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1743877799</td>\n",
       "      <td>10540337</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>59.60</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>61.02</td>\n",
       "      <td>78.29</td>\n",
       "      <td>66.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time        TO  sensor2_min  sensor2_max  sensor2_mean  sensor3_min  \\\n",
       "0  1743532199  10540337         10.0         98.0         55.59         12.0   \n",
       "1  1743618599  10540337         10.0         98.0         57.70         10.0   \n",
       "2  1743704999  10540337         10.0         99.0         50.29         10.0   \n",
       "3  1743791399  10540337         10.0         99.0         58.97         11.0   \n",
       "4  1743877799  10540337         10.0         99.0         59.60         10.0   \n",
       "\n",
       "   sensor3_max  sensor3_mean  sensor1_min  sensor1_max  \n",
       "0         99.0         54.11        65.83        65.27  \n",
       "1         99.0         51.68        76.09        54.77  \n",
       "2         98.0         53.25        67.66        66.12  \n",
       "3         98.0         49.92        83.22        80.61  \n",
       "4         99.0         61.02        78.29        66.14  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34d577ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1757527232_1757278522_part2.parquet', '1757527232_1757278522_part1.parquet']\n",
      "['1757565217_1757565282.parquet', '1757565217_1757565295.parquet', '1757527232_1757278522.parquet']\n",
      "f:  1757527232_1757278522_part2.parquet\n",
      "f:  1757527232_1757278522_part1.parquet\n",
      "(296, 10)\n",
      "         time        TO  sensor1_min  sensor1_max  sensor2_min  sensor2_max  \\\n",
      "0  1743532199  10540337         40.0         60.0         10.0         98.0   \n",
      "1  1743618599  10032096         40.5         60.7         10.5         99.0   \n",
      "2  1743704999  20012001         41.0         61.4         11.0         98.0   \n",
      "3  1743791399  30045009         41.5         62.1         10.0         99.0   \n",
      "4  1743877799  10540337         42.0         62.8         10.5         98.0   \n",
      "\n",
      "   sensor2_mean  sensor3_min  sensor3_max  sensor3_mean  \n",
      "0          50.0         10.0         99.0          52.0  \n",
      "1          50.6         11.0         98.0          52.4  \n",
      "2          51.2         12.0         97.0          52.8  \n",
      "3          51.8         13.0         96.0          53.2  \n",
      "4          52.4         14.0         99.0          53.6  \n"
     ]
    }
   ],
   "source": [
    "# Now, let's read and merge parquet files from a folder and \n",
    "# maintain the order of operations for schema consistency\n",
    "# \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = \"../data/artifacts2/reqid_001\"\n",
    "parquet_files = [f for f in os.listdir(folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "# Separate part files and full files\n",
    "part_files = [f for f in parquet_files if \"_part\" in f]\n",
    "print(part_files)\n",
    "full_files = [f for f in parquet_files if \"_part\" not in f]\n",
    "print(full_files)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Handle full files\n",
    "for f in full_files:\n",
    "    df = pd.read_parquet(os.path.join(folder, f))\n",
    "    dfs.append(df)\n",
    "\n",
    "# Handle part files: group by prefix before '_part'\n",
    "from collections import defaultdict\n",
    "part_groups = defaultdict(list)\n",
    "for f in part_files:\n",
    "    print(\"f: \", f)\n",
    "    prefix = f.split(\"_part\")[0]\n",
    "    part_groups[prefix].append(f)\n",
    "\n",
    "for prefix, files in part_groups.items():\n",
    "    # Sort to ensure consistent order\n",
    "    files = sorted(files)\n",
    "    part_dfs = [pd.read_parquet(os.path.join(folder, f)) for f in files]\n",
    "    # Merge column-wise (axis=1)\n",
    "    merged_df = pd.concat(part_dfs, axis=1)\n",
    "    # Remove duplicate columns if any (e.g., 'time', 'TO')\n",
    "    merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "    dfs.append(merged_df)\n",
    "\n",
    "# # Handle full files\n",
    "# for f in full_files:\n",
    "#     df = pd.read_parquet(os.path.join(folder, f))\n",
    "#     dfs.append(df)\n",
    "\n",
    "# Combine all data vertically (axis=0)\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(final_df.shape)\n",
    "\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce64cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part files in ../data/artifacts2/reqid_001: ['1757527232_1757278522_part2.parquet', '1757527232_1757278522_part1.parquet']\n",
      "Full files in ../data/artifacts2/reqid_001: ['1757565217_1757565282.parquet', '1757565217_1757565295.parquet', '1757527232_1757278522.parquet']\n",
      "f:  1757527232_1757278522_part2.parquet\n",
      "inside for loop, folder:  ../data/artifacts2/reqid_001\n",
      "f:  1757527232_1757278522_part1.parquet\n",
      "inside for loop, folder:  ../data/artifacts2/reqid_001\n",
      "Part files in ../data/artifacts2/reqid002: []\n",
      "Full files in ../data/artifacts2/reqid002: ['1757527232_1757278782.parquet', '1757527232_1757278781.parquet']\n",
      "(460, 10)\n",
      "         time        TO  sensor1_min  sensor1_max  sensor2_min  sensor2_max  \\\n",
      "0  1743532199  10540337         40.0         60.0         10.0         98.0   \n",
      "1  1743618599  10032096         40.5         60.7         10.5         99.0   \n",
      "2  1743704999  20012001         41.0         61.4         11.0         98.0   \n",
      "3  1743791399  30045009         41.5         62.1         10.0         99.0   \n",
      "4  1743877799  10540337         42.0         62.8         10.5         98.0   \n",
      "\n",
      "   sensor2_mean  sensor3_min  sensor3_max  sensor3_mean  \n",
      "0          50.0         10.0         99.0          52.0  \n",
      "1          50.6         11.0         98.0          52.4  \n",
      "2          51.2         12.0         97.0          52.8  \n",
      "3          51.8         13.0         96.0          53.2  \n",
      "4          52.4         14.0         99.0          53.6  \n"
     ]
    }
   ],
   "source": [
    "# Now, let's read and merge parquet files from multiple folders and \n",
    "# maintain the order of operations for schema consistency\n",
    "# \n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "folders = [\"../data/artifacts2/reqid_001\", \"../data/artifacts2/reqid002\"]\n",
    "dfs = []\n",
    "\n",
    "for folder in folders:\n",
    "    parquet_files = [f for f in os.listdir(folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "    # Separate part files and full files\n",
    "    part_files = [f for f in parquet_files if \"_part\" in f]\n",
    "    print(f\"Part files in {folder}:\", part_files)\n",
    "    full_files = [f for f in parquet_files if \"_part\" not in f]\n",
    "    print(f\"Full files in {folder}:\", full_files)\n",
    "\n",
    "    # Handle full files\n",
    "    for f in full_files:\n",
    "        df = pd.read_parquet(os.path.join(folder, f))\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Handle part files: group by prefix before '_part'\n",
    "    part_groups = defaultdict(list)\n",
    "    for f in part_files:\n",
    "        print(\"f: \", f)\n",
    "        print(\"inside for loop, folder: \", folder)\n",
    "        prefix = f.split(\"_part\")[0]\n",
    "        part_groups[prefix].append(f)\n",
    "\n",
    "    for prefix, files in part_groups.items():\n",
    "        # Sort to ensure consistent order\n",
    "        files = sorted(files)\n",
    "        part_dfs = [pd.read_parquet(os.path.join(folder, f)) for f in files]\n",
    "        # Merge column-wise (axis=1)\n",
    "        merged_df = pd.concat(part_dfs, axis=1)\n",
    "        # Remove duplicate columns if any (e.g., 'time', 'TO')\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "        dfs.append(merged_df)\n",
    "\n",
    "# Combine all data vertically (axis=0)\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(final_df.shape)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9020f7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip files found: ['reqid002.zip', 'reqid_001.zip']\n",
      "Processing ../data/artifacts2/reqid002.zip...\n",
      "Files in zip: ['reqid002/', 'reqid002/1757527232_1757278782.parquet', 'reqid002/1757527232_1757278781.parquet']\n",
      "Processing ../data/artifacts2/reqid_001.zip...\n",
      "Files in zip: ['reqid_001/', 'reqid_001/1757565217_1757565282.parquet', 'reqid_001/1757527232_1757278522_part2.parquet', 'reqid_001/1757527232_1757278522_part1.parquet', 'reqid_001/1757565217_1757565295.parquet', 'reqid_001/1757527232_1757278522.parquet']\n",
      "Full files: ['reqid_001/1757565217_1757565282.parquet', 'reqid_001/1757565217_1757565295.parquet', 'reqid_001/1757527232_1757278522.parquet']\n",
      "Part groups: defaultdict(<class 'list'>, {'1757527232_1757278522': ['reqid_001/1757527232_1757278522_part2.parquet', 'reqid_001/1757527232_1757278522_part1.parquet']})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from collections import defaultdict\n",
    "\n",
    "artifacts_folder = \"../data/artifacts2\"\n",
    "dfs = []\n",
    "\n",
    "# Find all zip files in the artifacts folder\n",
    "zip_files = [f for f in os.listdir(artifacts_folder) if f.endswith(\".zip\")]\n",
    "print(\"Zip files found:\", zip_files)\n",
    "\n",
    "for zip_filename in zip_files:\n",
    "    zip_path = os.path.join(artifacts_folder, zip_filename)\n",
    "    print(f\"Processing {zip_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        print(\"Files in zip:\", z.namelist())\n",
    "        # Group part files by prefix, collect full files\n",
    "        part_groups = defaultdict(list)\n",
    "        full_files = []\n",
    "        for file in z.namelist():\n",
    "            if file.endswith(\".parquet\"):\n",
    "                if \"_part\" in os.path.basename(file):\n",
    "                    prefix = os.path.basename(file).split(\"_part\")[0]\n",
    "                    part_groups[prefix].append(file)\n",
    "                else:\n",
    "                    full_files.append(file)\n",
    "\n",
    "print(\"Full files:\", full_files)\n",
    "print(\"Part groups:\", part_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c40971b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip files found: ['reqid002.zip', 'reqid_001.zip']\n",
      "Processing ../data/artifacts2/reqid002.zip...\n",
      "Files in zip: ['reqid002/', 'reqid002/1757527232_1757278782.parquet', 'reqid002/1757527232_1757278781.parquet']\n",
      "Processing ../data/artifacts2/reqid_001.zip...\n",
      "Files in zip: ['reqid_001/', 'reqid_001/1757565217_1757565282.parquet', 'reqid_001/1757527232_1757278522_part2.parquet', 'reqid_001/1757527232_1757278522_part1.parquet', 'reqid_001/1757565217_1757565295.parquet', 'reqid_001/1757527232_1757278522.parquet']\n",
      "(378, 10)\n",
      "         time        TO  sensor1_min  sensor1_max  sensor2_min  sensor2_max  \\\n",
      "0  1743532199  10540337        65.83        65.27         10.0         98.0   \n",
      "1  1743618599  10540337        76.09        54.77         10.0         98.0   \n",
      "2  1743704999  10540337        67.66        66.12         10.0         99.0   \n",
      "3  1743791399  10540337        83.22        80.61         10.0         99.0   \n",
      "4  1743877799  10540337        78.29        66.14         10.0         99.0   \n",
      "\n",
      "   sensor2_mean  sensor3_min  sensor3_max  sensor3_mean  \n",
      "0         55.59         12.0         99.0         54.11  \n",
      "1         57.70         10.0         99.0         51.68  \n",
      "2         50.29         10.0         98.0         53.25  \n",
      "3         58.97         11.0         98.0         49.92  \n",
      "4         59.60         10.0         99.0         61.02  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from collections import defaultdict\n",
    "\n",
    "artifacts_folder = \"../data/artifacts2\"\n",
    "dfs = []\n",
    "\n",
    "# Find all zip files in the artifacts folder\n",
    "zip_files = [f for f in os.listdir(artifacts_folder) if f.endswith(\".zip\")]\n",
    "print(\"Zip files found:\", zip_files)\n",
    "\n",
    "for zip_filename in zip_files:\n",
    "    zip_path = os.path.join(artifacts_folder, zip_filename)\n",
    "    print(f\"Processing {zip_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        print(\"Files in zip:\", z.namelist())\n",
    "        # Group part files by prefix, collect full files\n",
    "        part_groups = defaultdict(list)\n",
    "        full_files = []\n",
    "        for file in z.namelist():\n",
    "            if file.endswith(\".parquet\"):\n",
    "                if \"_part\" in os.path.basename(file):\n",
    "                    prefix = os.path.basename(file).split(\"_part\")[0]\n",
    "                    part_groups[prefix].append(file)\n",
    "                else:\n",
    "                    full_files.append(file)\n",
    "        # Handle full files\n",
    "        for file in full_files:\n",
    "            with z.open(file) as f:\n",
    "                df = pd.read_parquet(BytesIO(f.read()))\n",
    "                dfs.append(df)\n",
    "\n",
    "# Combine all data vertically (axis=0)\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(final_df.shape)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe39131",
   "metadata": {},
   "source": [
    "### BytesIO\n",
    "BytesIO is used to treat the bytes read from a file-like object (such as a file inside a zip archive) as a file in memory.\n",
    "\n",
    "When you use z.open(file) on a zipfile, it returns a file-like object, but pd.read_parquet() expects a file path or a file-like object that supports random access (seek/tell). Wrapping the bytes with BytesIO(f.read()) creates an in-memory binary stream that pandas can read as if it were a regular file.\n",
    "\n",
    "In summary:\n",
    "\n",
    "BytesIO(f.read()) allows you to read a Parquet file directly from a zip archive without extracting it to disk.\n",
    "It converts the bytes from the zip file into a file-like object that pandas can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf7843f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip files found: ['reqid002.zip', 'reqid_001.zip']\n",
      "Processing ../data/artifacts2/reqid002.zip...\n",
      "Files in zip: ['reqid002/', 'reqid002/1757527232_1757278782.parquet', 'reqid002/1757527232_1757278781.parquet']\n",
      "Processing ../data/artifacts2/reqid_001.zip...\n",
      "Files in zip: ['reqid_001/', 'reqid_001/1757565217_1757565282.parquet', 'reqid_001/1757527232_1757278522_part2.parquet', 'reqid_001/1757527232_1757278522_part1.parquet', 'reqid_001/1757565217_1757565295.parquet', 'reqid_001/1757527232_1757278522.parquet']\n",
      "(460, 10)\n",
      "         time        TO  sensor1_min  sensor1_max  sensor2_min  sensor2_max  \\\n",
      "0  1743532199  10540337        65.83        65.27         10.0         98.0   \n",
      "1  1743618599  10540337        76.09        54.77         10.0         98.0   \n",
      "2  1743704999  10540337        67.66        66.12         10.0         99.0   \n",
      "3  1743791399  10540337        83.22        80.61         10.0         99.0   \n",
      "4  1743877799  10540337        78.29        66.14         10.0         99.0   \n",
      "\n",
      "   sensor2_mean  sensor3_min  sensor3_max  sensor3_mean  \n",
      "0         55.59         12.0         99.0         54.11  \n",
      "1         57.70         10.0         99.0         51.68  \n",
      "2         50.29         10.0         98.0         53.25  \n",
      "3         58.97         11.0         98.0         49.92  \n",
      "4         59.60         10.0         99.0         61.02  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from collections import defaultdict\n",
    "\n",
    "artifacts_folder = \"../data/artifacts2\"\n",
    "dfs = []\n",
    "\n",
    "# Find all zip files in the artifacts folder\n",
    "zip_files = [f for f in os.listdir(artifacts_folder) if f.endswith(\".zip\")]\n",
    "print(\"Zip files found:\", zip_files)\n",
    "\n",
    "for zip_filename in zip_files:\n",
    "    zip_path = os.path.join(artifacts_folder, zip_filename)\n",
    "    print(f\"Processing {zip_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        print(\"Files in zip:\", z.namelist())\n",
    "        # Group part files by prefix, collect full files\n",
    "        part_groups = defaultdict(list)\n",
    "        full_files = []\n",
    "        for file in z.namelist():\n",
    "            if file.endswith(\".parquet\"):\n",
    "                if \"_part\" in os.path.basename(file):\n",
    "                    prefix = os.path.basename(file).split(\"_part\")[0]\n",
    "                    part_groups[prefix].append(file)\n",
    "                else:\n",
    "                    full_files.append(file)\n",
    "        # Handle full files\n",
    "        for file in full_files:\n",
    "            with z.open(file) as f:\n",
    "                df = pd.read_parquet(BytesIO(f.read()))\n",
    "                dfs.append(df)\n",
    "        # Handle part files\n",
    "        for prefix, files in part_groups.items():\n",
    "            files = sorted(files)\n",
    "            part_dfs = []\n",
    "            for file in files:\n",
    "                with z.open(file) as f:\n",
    "                    part_dfs.append(pd.read_parquet(BytesIO(f.read())))\n",
    "            merged_df = pd.concat(part_dfs, axis=1)\n",
    "            merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "            dfs.append(merged_df)\n",
    "\n",
    "# Combine all data vertically (axis=0)\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(final_df.shape)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6865284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
